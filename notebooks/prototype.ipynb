{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import List\n",
    "\n",
    "if False:\n",
    "    from pointnet2.pointnet2_utils import (\n",
    "        ball_query, gather_operation, furthest_point_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: To compute voxel_size, multiply base size (specified in SECOND config) by downsampling ratio.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    n_keypoints = 2048\n",
    "    strides = [1, 2, 4, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSA_MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents G in equation 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: List):\n",
    "        \"\"\"\n",
    "        channels: length-3 list of channels in each layer.\n",
    "        \"\"\"\n",
    "        self.linear = lambda x: x\n",
    "\n",
    "    def forward(self, voxel_set: torch.Tensor):\n",
    "        x = self.linear(voxel_set).max(2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VoxelSetAbstraction(nn.Module):\n",
    "    \"\"\"\n",
    "    For each keypoint, convert its location to\n",
    "    continuous voxel index coordinates. Then fetch\n",
    "    voxels within ball query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, radius: float, nsample: int, voxel_size: torch.Tensor, volume_offset: torch.Tensor):\n",
    "        \"\"\"\n",
    "        radius: maximum distance for ball query, measured in raw point cloud coordinates.\n",
    "        nsample: maximum number of neighbors to return in ball query.\n",
    "        voxel_size: length-3 tensor describing size of atomic voxel, accounting for stride.\n",
    "        volume_offset: length-3 tensor describing coordinate offset of voxel grid.\n",
    "        \"\"\"\n",
    "        # TODO: call super\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.voxel_size = voxel_size\n",
    "        self.volume_offset = volume_offset\n",
    "\n",
    "    def to_raw_coordinates(self, voxel_index: torch.Tensor):\n",
    "        \"\"\"\n",
    "        voxel_index: shape (B, Tk, 3) array of coordinates\n",
    "        return: shape (B, Tk, 3) array of locations in raw coordinates.\n",
    "        \"\"\"\n",
    "        location = (voxel_index * self.voxel_size) + self.volume_offset\n",
    "        location = location / self.voxel_size\n",
    "        return location\n",
    "\n",
    "    def get_neighbors(self, keypoint_location: torch.Tensor, voxel_feature: torch.Tensor, voxel_location: torch.Tensor)\n",
    "        neighbor_index = ball_query(self.radius, self.nsample, voxel_location, keypoint_location)\n",
    "        neighbor_feature = gather_operation(voxel_feature, neighbor_index)\n",
    "        neighbor_location = gather_operation(voxel_location, neighbor_index)\n",
    "        return neighbor_feature, neighbor_location\n",
    "\n",
    "    def combine_features(self, neighbor_feature: torch.Tensor, neighbor_location: torch.Tensor, keypoint_location: torch.Tensor):\n",
    "        \"\"\"Form neighborhood feature set (equation 1).\"\"\"\n",
    "        offset_location = neighbor_location - keypoint_location\n",
    "        combined_feature = torch.cat((neighbor_feature, offset_location), dim=2)\n",
    "        return combined_feature\n",
    "\n",
    "    def forward(self, keypoint_location: torch.Tensor, voxel_feature: torch.Tensor, voxel_index: torch.Tensor):\n",
    "        voxel_location = self.to_raw_coordinates(voxel_index)\n",
    "        neighbor_feature, neighbor_location = self.ball_query(keypoint_location, voxel_feature, voxel_location)\n",
    "        feature = self.combine_features(neighbor_feature, neighbor_location, keypoint_location)\n",
    "        return feature\n",
    "\n",
    "\n",
    "class PV_RCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Carry out feature computation described in PV-RCNN paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_keypoint: int):\n",
    "        \"\"\"\n",
    "        num_keypoint: number of keypoints\n",
    "        \"\"\"\n",
    "        self.num_keypoint = num_keypoint\n",
    "        pass\n",
    "    \n",
    "    def forward(self, raw_point):\n",
    "        keypoint_index = furthest_point_sample(raw_point, self.num_keypoint)\n",
    "        keypoint = gather_operation(raw_point, keypoint_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C = 2, 18000\n",
    "x = torch.randn((B, C, 4), dtype=torch.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
