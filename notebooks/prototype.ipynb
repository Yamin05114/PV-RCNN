{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import spconv\n",
    "import sys\n",
    "sys.path.append('/home/jhultman/Projects/tmp/PV-RCNN/Pointnet2.PyTorch/')\n",
    "from pointnet2.pointnet2_utils import ball_query, gather_operation, furthest_point_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PvrcnnConfig:\n",
    "    C_in = 4\n",
    "    n_keypoints = 2048\n",
    "    strides = [1, 2, 4, 8]\n",
    "    max_num_points = 5\n",
    "    max_voxels = 40000\n",
    "    voxel_size = [0.05, 0.05, 0.1]\n",
    "    grid_bounds = [0, -40, -3, 64, 40, 1]\n",
    "    sample_fpath = './sample.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSA_MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents G in equation 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C_in, channels):\n",
    "        \"\"\"\n",
    "        C_in: incoming channels.\n",
    "        channels: length-3 list of channels in each layer.\n",
    "        \"\"\"\n",
    "        super(VSA_MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(C_in, channels[0], bias=True),\n",
    "            nn.BatchNorm1d(channels[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels[0], channels[1], bias=True),\n",
    "            nn.BatchNorm1d(channels[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels[1], channels[2], bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, voxel_set):\n",
    "        x = self.layers(voxel_set)\n",
    "        x = x.max(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelSetAbstraction(nn.Module):\n",
    "    \"\"\"\n",
    "    For each keypoint, convert its location to\n",
    "    continuous voxel index coordinates. Then fetch\n",
    "    voxels within ball query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, radius, nsample, voxel_size, volume_offset):\n",
    "        \"\"\"\n",
    "        radius: maximum distance for ball query, measured in raw point cloud coordinates.\n",
    "        nsample: maximum number of neighbors to return in ball query.\n",
    "        voxel_size: length-3 tensor describing size of atomic voxel, accounting for stride.\n",
    "        volume_offset: length-3 tensor describing coordinate offset of voxel grid.\n",
    "        \"\"\"\n",
    "        super(VoxelSetAbstraction, self).__init__()\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.voxel_size = voxel_size\n",
    "        self.volume_offset = volume_offset\n",
    "\n",
    "    def to_raw_coordinates(self, voxel_index):\n",
    "        \"\"\"\n",
    "        voxel_index: shape (B, Tk, 4) array of coordinates\n",
    "        return: shape (B, Tk, 3) array of locations in raw coordinates.\n",
    "        \"\"\"\n",
    "        xyz = voxel_index[..., 1:].float() * self.voxel_size\n",
    "        xyz = (xyz + self.volume_offset).unsqueeze(0)\n",
    "        return xyz\n",
    "\n",
    "    def get_neighbors(self, keypoint_location, voxel_feature, voxel_location):\n",
    "        neighbor_index = ball_query(self.radius, self.nsample, voxel_location, keypoint_location)\n",
    "        neighbor_feature = gather_operation(voxel_feature, neighbor_index)\n",
    "        neighbor_location = gather_operation(voxel_location, neighbor_index)\n",
    "        return neighbor_feature, neighbor_location\n",
    "\n",
    "    def combine_features(self, neighbor_feature, neighbor_location, keypoint_location):\n",
    "        \"\"\"Form neighborhood feature set (equation 1).\"\"\"\n",
    "        offset_location = neighbor_location - keypoint_location\n",
    "        combined_feature = torch.cat((neighbor_feature, offset_location), dim=2)\n",
    "        return combined_feature\n",
    "\n",
    "    def forward(self, keypoint_location, voxel_feature, voxel_index):\n",
    "        voxel_location = self.to_raw_coordinates(voxel_index)\n",
    "        neighbor_feature, neighbor_location = self.get_neighbors(keypoint_location, voxel_feature, voxel_location)\n",
    "        feature = self.combine_features(neighbor_feature, neighbor_location, keypoint_location)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Placeholder sparse 3D CNN with four blocks:\n",
    "    \n",
    "        block_0: [1600, 1280, 41] -> [1600, 1280, 41]\n",
    "        block_1: [1600, 1280, 41] -> [800, 640, 21]\n",
    "        block_2: [800, 640, 21]   -> [400, 320, 11]\n",
    "        block_3: [400, 320, 11]   -> [200, 160, 6]\n",
    "    \n",
    "    Returns feature volumes strided 1x, 2x, 4x, 8x.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C_in, shape):\n",
    "        super(CNN_3D, self).__init__()\n",
    "        self.blocks = spconv.SparseSequential(\n",
    "            spconv.SparseConv3d(C_in, 16, 3, 1, padding=0, bias=False),\n",
    "            spconv.SparseConv3d(16, 16, 3, 2, padding=1, bias=False),\n",
    "            spconv.SparseConv3d(16, 32, 3, 2, padding=1, bias=False),\n",
    "            spconv.SparseConv3d(32, 64, 3, 2, padding=1, bias=False),\n",
    "        )\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, features, coordinates, batch_size):\n",
    "        x0 = spconv.SparseConvTensor(\n",
    "            features, coordinates.int(), self.shape, batch_size,\n",
    "        )\n",
    "        x1 = self.blocks[0](x0)\n",
    "        x2 = self.blocks[1](x1)\n",
    "        x3 = self.blocks[2](x2)\n",
    "        x4 = self.blocks[3](x3)\n",
    "        x = [x1, x2, x3, x4]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PV_RCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Carry out feature computation described in PV-RCNN paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_keypoint):\n",
    "        \"\"\"\n",
    "        num_keypoint: number of keypoints\n",
    "        \"\"\"\n",
    "        super(PV_RCNN, self).__init__()\n",
    "        self.num_keypoint = num_keypoint\n",
    "        pass\n",
    "    \n",
    "    def forward(self, raw_point):\n",
    "        keypoint_index = furthest_point_sample(raw_point, self.num_keypoint)\n",
    "        keypoint = gather_operation(raw_point, keypoint_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = PvrcnnConfig()\n",
    "\n",
    "voxel_generator = spconv.utils.VoxelGenerator(\n",
    "    voxel_size=cfg.voxel_size, \n",
    "    point_cloud_range=cfg.grid_bounds,\n",
    "    max_voxels=cfg.max_voxels,\n",
    "    max_num_points=cfg.max_num_points,\n",
    ")\n",
    "\n",
    "points = np.fromfile(cfg.sample_fpath, dtype=np.float32).reshape(-1, cfg.C_in)\n",
    "features, coordinates, voxel_population = voxel_generator.generate(points)\n",
    "coordinates = np.pad(coordinates, ((0, 0), (1, 0)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "from_numpy = lambda x: torch.from_numpy(x).cuda()\n",
    "points, features, coordinates, voxel_population = map(\n",
    "    from_numpy, (points, features, coordinates, voxel_population))\n",
    "features = features.view(-1, cfg.C_in)\n",
    "\n",
    "shape = np.r_[voxel_generator.grid_size[::-1]] + [1, 0, 0] # [1280, 1600, 40] -> [41, 1600, 1280]\n",
    "cnn_3d = CNN_3D(C_in=cfg.C_in, shape=shape).cuda()\n",
    "out = cnn_3d(features.view(-1, 4), coordinates, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa = VoxelSetAbstraction(\n",
    "    radius=0.2, nsample=25, \n",
    "    voxel_size=torch.cuda.FloatTensor([0.05, 0.05, 0.1]),\n",
    "    volume_offset=torch.cuda.FloatTensor([0, 0, 0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_keypoint = furthest_point_sample(points.unsqueeze(0), cfg.n_keypoints)\n",
    "keypoint_location = points[indices_keypoint.long()]\n",
    "\n",
    "idx = 2\n",
    "voxel_size = torch.cuda.FloatTensor(cfg.voxel_size) * 2 ** (idx)\n",
    "volume_offset = torch.cuda.FloatTensor([0, 0, 0])\n",
    "\n",
    "feature_volume = out[idx]\n",
    "voxel_feature = feature_volume.features\n",
    "voxel_index = feature_volume.indices\n",
    "\n",
    "xyz = voxel_index[..., 1:].float() * voxel_size\n",
    "xyz = (xyz + volume_offset).unsqueeze(0)\n",
    "\n",
    "neighbor_index = ball_query(0.4, 8, xyz, keypoint_location)\n",
    "neighbor_feature = gather_operation(\n",
    "    voxel_feature.unsqueeze(0), \n",
    "    neighbor_index.squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 8])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 43009, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 8])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 415043, 16])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([415043, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 25])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([415043, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([415043, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def forward(self, keypoint_location, voxel_feature, voxel_index):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "tmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
